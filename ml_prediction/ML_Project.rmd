---
title: "Practical Machine Learning"
author: "Olga Koroleva"
output: pdf_document
subtitle: Prediction Assignment - Week 4
---
## Abstract
The goal of this project is to predict the manner in which people exercise based on accelerometers on the belt, forearm, arm, and dumbbell of 6 participants from the Weight Lifting Exercise Dataset using different machine learning algorithms.

Six participants were asked to perform barbell lifts correcty and incorrectly in five different manners wearing fitness trackers like Jawbone Up, Nike FuelBand, and Fitbit in this dataset. The data gained from these devices is used to train the models.

## Data Sources
The training data for this project are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source:
http://groupware.les.inf.puc-rio.br/har

## Data Import & Transformation
The outcome variable is classe, a factor variable with 5 levels. For this data set, participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in 5 different fashions:

exactly according to the specification (Class A)
throwing the elbows to the front (Class B)
lifting the dumbbell only halfway (Class C)
lowering the dumbbell only halfway (Class D)
throwing the hips to the front (Class E)

The initial configuration consists of loading some required packages and initializing some variables.

```{r}
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)

# Set seed for reproducability
set.seed(9999)

# Load Data
training   <-read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!", ""))
testing <-read.csv("pml-testing.csv" , na.strings=c("NA", "#DIV/0!", ""))
training<-training[,colSums(is.na(training)) == 0]
testing <-testing[,colSums(is.na(testing)) == 0]

# Subset data
training   <-training[,-c(1:7)]
testing <-testing[,-c(1:7)]
```
## Cross-Validation
In this section cross-validation will be performed by splitting the training data in training (75%) and testing (25%) data.
```{r}
subSamples <- createDataPartition(y=training$classe, p=0.75, list=FALSE)
subTraining <- training[subSamples, ]
subTesting <- training[-subSamples, ]
```
## Exploratory Analysis
The variable classe contains 5 levels. The plot of the outcome variable shows the frequency of each levels in the subTraining data.
```{r}
plot(as.factor(subTraining$classe), col="blue", main="Levels of the variable classe", xlab="classe levels", ylab="Frequency", )
```
The plot above shows that Level A is the most frequent classe. D appears to be the least frequent one.

## Prediction Models
In this section a decision tree and random forest will be applied to the data.

### Decision Tree
```{r}
# Fiting the model
modFitDT <- rpart(classe ~ ., data=subTraining, method="class")
# Performing prediction
predictDT <- predict(modFitDT, subTesting, type = "class")

rpart.plot(modFitDT, main="Classification Tree",  under=TRUE, faclen=0)

#The following confusion matrix shows the errors of the decision tree prediction.
confusionMatrix(predictDT, as.factor(subTesting$classe))

```

### Random Forest
```{r}
# Fiting the model
modFitRF <- randomForest(as.factor(classe)~., data=subTraining, method="class")

# Perform prediction
predictRF <- predict(modFitRF, subTesting, type = "class")

#The following confusion matrix shows the errors of random forest prediction.
confusionMatrix(predictRF,  as.factor(subTesting$classe))
```

## Conclusion
Result
The confusion matrices show, that the Random Forest algorithm performs better than decision trees. The accuracy for the Random Forest model was 0.995 (95% CI: (0.993, 0.997)) compared to 0.739 (95% CI: (0.727, 0.752)) for Decision Tree model. The random Forest model is chosen.

### Expected out-of-sample error
The expected out-of-sample error is estimated at 0.005, or 0.5%. The expected out-of-sample error is calculated as 1 - accuracy for predictions made against the cross-validation set. Our Test data set comprises 20 cases. With an accuracy above 99% on our cross-validation data, we can expect that very few, or none, of the test samples will be mis-classified.
